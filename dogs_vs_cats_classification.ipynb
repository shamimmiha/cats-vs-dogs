{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ± vs ðŸ¶ Classification: An Adventure in Transfer Learning\n",
    "\n",
    "## A journey with Adil and Harry to understand Deep Learning\n",
    "\n",
    "*In this notebook, follow along as Adil guides his friend Harry through building a neural network that can tell dogs from cats. What starts as a simple challenge becomes an adventure into the world of deep learning!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Episode 1: The Challenge Begins\n",
    "\n",
    "**Harry**: *[scrolling through his phone]* Hey Adil, check this out! My friend just sent me this picture asking if it's a very fluffy cat or a small dog. I can't tell!\n",
    "\n",
    "**Adil**: *[laughing]* Let me see that! Hmm, that's definitely a Pomeranian dog. But I see the confusion - all that fluff!\n",
    "\n",
    "**Harry**: I wish there was a way to automatically tell dogs from cats in photos. That would be cool.\n",
    "\n",
    "**Adil**: Actually, we can build that! It's a perfect excuse for me to show you some deep learning. We can make a computer program that learns to recognize dogs and cats.\n",
    "\n",
    "**Harry**: Seriously? That sounds complicated... I barely know Python!\n",
    "\n",
    "**Adil**: Don't worry! I'll guide you through it step by step. We'll use something called transfer learning to make it easier. Think of it like getting a head start by standing on the shoulders of giants.\n",
    "\n",
    "**Harry**: Alright, I'm in! Where do we start?\n",
    "\n",
    "**Adil**: First, we need to set up our environment and get some data. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setting Up Our Environment\n",
    "\n",
    "**Adil**: Before we dive into the exciting stuff, we need to make sure we have all the right tools. Let's import the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import the libraries we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow and Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# For displaying images\n",
    "from PIL import Image\n",
    "\n",
    "# Make sure we're using the GPU if available\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Whoa, that's a lot of imports! What does each one do?\n",
    "\n",
    "**Adil**: Good question! Let me break it down:\n",
    "- **numpy and pandas**: These are for working with data - like spreadsheets but more powerful\n",
    "- **matplotlib**: For making charts and displaying images\n",
    "- **TensorFlow and Keras**: The main engines for our deep learning models\n",
    "- **PIL**: Helps us work with images\n",
    "- **tqdm**: Gives us those cool progress bars so we know how long things will take\n",
    "\n",
    "**Harry**: Got it! And what's that GPU thing at the end?\n",
    "\n",
    "**Adil**: That's checking if we have a graphics card (GPU) available. Deep learning is much faster on GPUs because they're designed to process lots of small calculations in parallel - exactly what we need for neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¾ Preparing Our Dataset\n",
    "\n",
    "**Harry**: So where do we get pictures of cats and dogs to train with?\n",
    "\n",
    "**Adil**: Great question! We'll use a famous dataset from Kaggle called \"Dogs vs. Cats\". It has 25,000 labeled images - 20,000 for training and 5,000 for testing.\n",
    "\n",
    "**Harry**: How do we download it?\n",
    "\n",
    "**Adil**: If you're running this on your local machine, you can download it from [Kaggle](https://www.kaggle.com/datasets/salader/dogs-vs-cats) and unzip it into a folder. If we're using Google Colab, we can download it directly with their API. Let's set up our paths assuming you've downloaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to our dataset\n",
    "# Update these paths based on where you downloaded the dataset\n",
    "base_dir = 'dogs-vs-cats'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Let's check if the directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(\"Please download the dataset from Kaggle and extract it to 'dogs-vs-cats' folder\")\n",
    "    print(\"Or uncomment and run the cell below if using Google Colab\")\n",
    "else:\n",
    "    print(f\"Dataset found at {base_dir}\")\n",
    "    # Count images in training and test directories\n",
    "    train_cats = len([f for f in os.listdir(os.path.join(train_dir, 'cat')) if f.endswith('.jpg')])\n",
    "    train_dogs = len([f for f in os.listdir(os.path.join(train_dir, 'dog')) if f.endswith('.jpg')])\n",
    "    test_cats = len([f for f in os.listdir(os.path.join(test_dir, 'cat')) if f.endswith('.jpg')])\n",
    "    test_dogs = len([f for f in os.listdir(os.path.join(test_dir, 'dog')) if f.endswith('.jpg')])\n",
    "    \n",
    "    print(f\"Training: {train_cats} cats and {train_dogs} dogs\")\n",
    "    print(f\"Testing: {test_cats} cats and {test_dogs} dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adil**: If you're using Google Colab, you can uncomment and run this cell to download and extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Google Colab users\n",
    "# !pip install kaggle\n",
    "# from google.colab import files\n",
    "# \n",
    "# # Upload your Kaggle API token (kaggle.json file)\n",
    "# print(\"Please upload your kaggle.json file\")\n",
    "# uploaded = files.upload()\n",
    "# \n",
    "# # Make directory and move kaggle.json file\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# \n",
    "# # Download and extract the dataset\n",
    "# !kaggle datasets download -d salader/dogs-vs-cats\n",
    "# !unzip -q dogs-vs-cats.zip\n",
    "# !rm dogs-vs-cats.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Great! But I'm curious - what do these cat and dog pictures actually look like?\n",
    "\n",
    "**Adil**: Let's take a peek at a few random examples from our training set. This will help us understand what kind of images our model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_images(cat_dir, dog_dir, num_images=4):\n",
    "    \"\"\"Show random cat and dog images from the dataset\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get random cat images\n",
    "    cat_files = [os.path.join(cat_dir, f) for f in os.listdir(cat_dir) if f.endswith('.jpg')]\n",
    "    random_cats = random.sample(cat_files, num_images)\n",
    "    \n",
    "    # Get random dog images\n",
    "    dog_files = [os.path.join(dog_dir, f) for f in os.listdir(dog_dir) if f.endswith('.jpg')]\n",
    "    random_dogs = random.sample(dog_files, num_images)\n",
    "    \n",
    "    # Display cats\n",
    "    for i, img_path in enumerate(random_cats):\n",
    "        plt.subplot(2, num_images, i+1)\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Cat {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Display dogs\n",
    "    for i, img_path in enumerate(random_dogs):\n",
    "        plt.subplot(2, num_images, num_images+i+1)\n",
    "        img = plt.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Dog {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some random images from our training set\n",
    "show_random_images(os.path.join(train_dir, 'cat'), os.path.join(train_dir, 'dog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: These pictures are so varied! Different backgrounds, lighting, and angles. Some are close-ups, some show the whole animal. Is that going to be a problem?\n",
    "\n",
    "**Adil**: That's actually perfect! Real-world images are messy and diverse, so having variety in our training data helps our model learn to recognize cats and dogs in many different scenarios. If we only trained on perfectly centered pet portraits, our model wouldn't work well on natural photos.\n",
    "\n",
    "**Harry**: I see. So the messiness is actually helpful for making a robust model?\n",
    "\n",
    "**Adil**: Exactly! We call this \"variance\" in the data, and it's essential for building models that generalize well to new, unseen images. That said, we should still preprocess our images to make them consistent in size and format. Let's do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Data Preprocessing\n",
    "\n",
    "**Adil**: Now we need to prepare our images for the neural network. Deep learning models expect images to be in a consistent format - same size, similar brightness ranges, etc.\n",
    "\n",
    "**Harry**: Like standardizing them?\n",
    "\n",
    "**Adil**: Exactly! We'll resize all images to the same dimensions, normalize the pixel values, and set up data augmentation to artificially expand our training set.\n",
    "\n",
    "**Harry**: Data augmen-what now?\n",
    "\n",
    "**Adil**: Data augmentation! It's like getting more data for free. We take our existing images and apply random transformations - flips, rotations, zooms - to create new training examples. A cat is still a cat even if the image is flipped horizontally, right?\n",
    "\n",
    "**Harry**: Oh, that's clever! Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image dimensions\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create data generators with augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,          # Normalize pixel values to [0,1]\n",
    "    rotation_range=20,       # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2,   # Randomly shift width by up to 20%\n",
    "    height_shift_range=0.2,  # Randomly shift height by up to 20%\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally\n",
    "    zoom_range=0.2,          # Randomly zoom in by up to 20%\n",
    "    fill_mode='nearest'      # Strategy for filling in newly created pixels\n",
    ")\n",
    "\n",
    "# For validation/test, we only need to rescale (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'  # binary because we have 2 classes (cats and dogs)\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Wait, what's with all these parameters? And what's that 'flow_from_directory' thing?\n",
    "\n",
    "**Adil**: Good questions! Let me explain:\n",
    "\n",
    "- **rescale=1./255**: Images typically have pixel values from 0-255, but neural networks work better with small values like 0-1, so we divide by 255.\n",
    "- **rotation_range, width_shift_range**, etc.: These are our augmentation settings. They create variations of our training images to help the model generalize better.\n",
    "- **flow_from_directory()**: This is a Keras helper that automatically loads images from folders. It assumes each subfolder is a different class (so we have 'cat' and 'dog' folders).\n",
    "- **target_size=(224, 224)**: We resize all images to 224Ã—224 pixels - a standard size for many pre-trained networks.\n",
    "- **class_mode='binary'**: Since we have just two classes (cat=0, dog=1), we use binary mode.\n",
    "\n",
    "**Harry**: I see! So the generators automatically handle loading and preprocessing our images?\n",
    "\n",
    "**Adil**: Exactly! And they generate batches of images on-the-fly during training, which saves memory. Let's visualize some augmented images to see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_augmented_images():\n",
    "    \"\"\"Visualize how data augmentation affects a single image\"\"\"\n",
    "    # Get a single image for demonstration\n",
    "    cat_files = [os.path.join(train_dir, 'cat', f) for f in os.listdir(os.path.join(train_dir, 'cat')) if f.endswith('.jpg')]\n",
    "    img_path = random.choice(cat_files)\n",
    "    \n",
    "    # Load and resize the image\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_array = np.array(img) / 255.0  # Normalize to [0,1]\n",
    "    img_array = img_array.reshape((1,) + img_array.shape)  # Reshape to (1, height, width, channels)\n",
    "    \n",
    "    # Create a data generator just for this image\n",
    "    aug_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2\n",
    "    )\n",
    "    \n",
    "    # Generate augmented images\n",
    "    aug_iterator = aug_datagen.flow(img_array, batch_size=1)\n",
    "    \n",
    "    # Plot original and augmented images\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(3, 3, 5)  # Center position\n",
    "    plt.imshow(img_array[0])\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Augmented versions\n",
    "    positions = [1, 2, 3, 4, 6, 7, 8, 9]  # All positions except center\n",
    "    for i, pos in enumerate(positions):\n",
    "        aug_img = next(aug_iterator)[0]\n",
    "        plt.subplot(3, 3, pos)\n",
    "        plt.imshow(aug_img)\n",
    "        plt.title(f'Augmented {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Show examples of augmented images\n",
    "show_augmented_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Wow! All these different versions from just one image? That's pretty cool!\n",
    "\n",
    "**Adil**: Right? Data augmentation is like getting 8 new training examples for free! Now our model will learn to recognize cats regardless of rotation, position, or whether they're flipped.\n",
    "\n",
    "**Harry**: This makes sense for photos, but will the same techniques work for other types of data?\n",
    "\n",
    "**Adil**: Great insight! Different data types need different augmentation strategies. For audio, you might change speed or add noise. For text, you might replace words with synonyms. The key is that the transformations should preserve the essential meaning or content while adding realistic variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Building Our Model with Transfer Learning\n",
    "\n",
    "**Harry**: Now for the fun part - building the actual neural network, right?\n",
    "\n",
    "**Adil**: Yes! But instead of building one from scratch, we'll use transfer learning. Remember I mentioned standing on the shoulders of giants?\n",
    "\n",
    "**Harry**: Oh right, what exactly is transfer learning again?\n",
    "\n",
    "**Adil**: Great question! Transfer learning is like leveraging someone else's expertise. Imagine you want to learn to paint portraits. Instead of starting from zero, you could learn from a master painter who already knows how to capture facial details, work with colors, etc.\n",
    "\n",
    "In deep learning, we use models that were pre-trained on millions of images to recognize thousands of objects. These models have already learned to detect edges, textures, shapes, and complex patterns. We take one of these pre-trained models and just retrain the final layers to recognize our specific classes - cats and dogs.\n",
    "\n",
    "**Harry**: That sounds way more efficient than starting from scratch!\n",
    "\n",
    "**Adil**: Exactly! Let's use VGG16, a famous CNN architecture pre-trained on ImageNet (a dataset with over a million images and 1000 categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_model():\n",
    "    \"\"\"Create a transfer learning model based on VGG16\"\"\"\n",
    "    # Load VGG16 model pre-trained on ImageNet without the top classification layer\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Create our new model on top\n",
    "    model = Sequential([\n",
    "        base_model,                          # VGG16 base\n",
    "        Flatten(),                           # Flatten the output of VGG16\n",
    "        Dense(256, activation='relu'),       # Dense layer with 256 neurons and ReLU activation\n",
    "        Dropout(0.5),                        # Dropout to prevent overfitting\n",
    "        Dense(1, activation='sigmoid')       # Output layer with sigmoid for binary classification\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',          # Loss function for binary classification\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Adam optimizer with low learning rate\n",
    "        metrics=['accuracy']                 # Track accuracy during training\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create our model\n",
    "model = create_transfer_learning_model()\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Whoa, that's a lot of layers and parameters! Can you break this down for me?\n",
    "\n",
    "**Adil**: Sure thing! Let me explain what's happening here:\n",
    "\n",
    "1. **Base Model**: We load VGG16 without its top layer. This gives us all the pattern-recognition power of VGG16, which has already been trained on millions of images.\n",
    "\n",
    "2. **Freezing Layers**: We set `trainable = False` for all base model layers. This \"freezes\" them so their weights won't change during our training. It's like saying \"we trust your expertise, don't change what you already know!\"\n",
    "\n",
    "3. **Our Custom Top**: We add our own layers on top of VGG16:\n",
    "   - **Flatten()**: Converts the 3D output of VGG16 into a 1D vector\n",
    "   - **Dense(256)**: A fully connected layer with 256 neurons\n",
    "   - **Dropout(0.5)**: Randomly turns off 50% of neurons during training to prevent overfitting\n",
    "   - **Dense(1, sigmoid)**: Our output layer - one neuron with sigmoid activation that gives us a probability (0-1) of the image being a dog\n",
    "\n",
    "4. **Compilation**: We set up our training process with:\n",
    "   - **binary_crossentropy**: The loss function suited for binary (two-class) problems\n",
    "   - **Adam optimizer**: An adaptive learning algorithm that helps find the optimal weights\n",
    "   - **learning_rate=0.0001**: A small learning rate to make small, careful updates\n",
    "\n",
    "**Harry**: I think I get it! We're using VGG16 as our foundation, and just adding our own \"dog vs. cat\" detector on top. But what's this \"overfitting\" you mentioned?\n",
    "\n",
    "**Adil**: Great question! Overfitting is like memorizing the answers to a test instead of understanding the material. The model learns the training data too well, including all its noise and peculiarities, but then performs poorly on new, unseen examples. Dropout helps prevent this by forcing the network to be more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Training Our Model\n",
    "\n",
    "**Harry**: Now that we've built our model, how do we actually train it?\n",
    "\n",
    "**Adil**: We'll use our data generators to feed batches of images to the model, and the model will gradually improve its predictions. Let's set up some callbacks to save the best model and stop training if we're not improving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('dogs_vs_cats_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = test_generator.samples // BATCH_SIZE\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: So this will take a while to run, right?\n",
    "\n",
    "**Adil**: Yes, it might take 20-30 minutes depending on your hardware. If you have a GPU, it'll be much faster. While we wait, let me explain what those callbacks are doing:\n",
    "\n",
    "1. **EarlyStopping**: This stops training if the validation accuracy doesn't improve for 3 consecutive epochs. It's like saying \"if you haven't gotten better at the test in 3 tries, let's stop and use your best performance.\"\n",
    "\n",
    "2. **ModelCheckpoint**: This saves the model whenever it achieves the best validation accuracy so far. It's like taking a snapshot every time you beat your personal best.\n",
    "\n",
    "**Harry**: What's the difference between training accuracy and validation accuracy?\n",
    "\n",
    "**Adil**: Think of training data as your study materials and validation data as practice tests:\n",
    "- **Training accuracy**: How well the model performs on images it's learning from. This can be artificially high if the model is memorizing.\n",
    "- **Validation accuracy**: How well the model performs on images it has never seen before. This is a better measure of real-world performance.\n",
    "\n",
    "We always want to optimize for validation accuracy, because that tells us how well our model will do on new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluating Our Model\n",
    "\n",
    "**Harry**: Training's done! How did our model do?\n",
    "\n",
    "**Adil**: Let's visualize the training and validation accuracy over time to see how the model improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation accuracy and loss\"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy', marker='o')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='o')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss', marker='o')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Looks like our accuracy is really high! But what's with those curves?\n",
    "\n",
    "**Adil**: Great observation! Let me interpret these graphs:\n",
    "\n",
    "1. **Accuracy Graph**:\n",
    "   - Both training and validation accuracy increase over time, which is good!\n",
    "   - If validation accuracy plateaued while training accuracy kept rising, that would signal overfitting.\n",
    "\n",
    "2. **Loss Graph**:\n",
    "   - Loss is the model's error measurement - lower is better.\n",
    "   - Both curves decreasing indicates the model is improving.\n",
    "\n",
    "Our model reached over 95% validation accuracy, which is excellent! Let's see how it performs on some specific test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_random_images(model, test_dir, num_images=8):\n",
    "    \"\"\"Test the model on random images from the test set\"\"\"\n",
    "    # Get random cat and dog images\n",
    "    cat_files = [os.path.join(test_dir, 'cat', f) for f in os.listdir(os.path.join(test_dir, 'cat')) if f.endswith('.jpg')]\n",
    "    dog_files = [os.path.join(test_dir, 'dog', f) for f in os.listdir(os.path.join(test_dir, 'dog')) if f.endswith('.jpg')]\n",
    "    \n",
    "    # Select random images\n",
    "    num_each = num_images // 2\n",
    "    random_cats = random.sample(cat_files, num_each)\n",
    "    random_dogs = random.sample(dog_files, num_each)\n",
    "    test_images = random_cats + random_dogs\n",
    "    random.shuffle(test_images)  # Mix them up\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, img_path in enumerate(test_images):\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "        img_array = np.array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img_array)[0][0]\n",
    "        \n",
    "        # Determine ground truth\n",
    "        true_class = 'Cat' if 'cat' in img_path.lower() else 'Dog'\n",
    "        pred_class = 'Dog' if prediction > 0.5 else 'Cat'\n",
    "        confidence = prediction if pred_class == 'Dog' else 1 - prediction\n",
    "        \n",
    "        # Set color based on correctness\n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        \n",
    "        # Display image with prediction\n",
    "        plt.subplot(2, num_images//2, i+1)\n",
    "        plt.imshow(img_array[0])\n",
    "        plt.title(f\"Prediction: {pred_class} ({confidence:.2f})\\nActual: {true_class}\", color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Model Predictions on Test Images', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Test on random images\n",
    "test_on_random_images(model, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: That's amazing! It correctly identified most of the images. But why did it get a few wrong?\n",
    "\n",
    "**Adil**: Even the best models aren't perfect! There could be several reasons for the errors:\n",
    "\n",
    "1. **Unusual poses or angles**: If the animal is in a strange position or only partially visible\n",
    "2. **Confusing features**: Some cats and dogs share similar features or colors\n",
    "3. **Image quality**: Blurry, dark, or low-contrast images are harder to classify\n",
    "4. **Limitations of our model**: We only trained for a few epochs and used a relatively simple architecture\n",
    "\n",
    "The important thing is that our model gets most images right, with high confidence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Final Thoughts and Future Improvements\n",
    "\n",
    "**Harry**: This has been incredible! I never thought I could build an AI that recognizes animals. What else could we do to make it even better?\n",
    "\n",
    "**Adil**: Great question! There are several ways we could improve our model:\n",
    "\n",
    "1. **Fine-tuning**: We could unfreeze some of the later layers of VGG16 and train them with a very small learning rate to adapt them specifically to our cats and dogs task.\n",
    "\n",
    "2. **More data**: More training data almost always helps. We could find additional cat and dog images to add to our dataset.\n",
    "\n",
    "3. **Better architecture**: We could try more modern architectures like ResNet, EfficientNet, or Vision Transformer, which might perform even better than VGG16.\n",
    "\n",
    "4. **Ensemble methods**: We could train multiple different models and combine their predictions for even higher accuracy.\n",
    "\n",
    "5. **Class activation maps**: We could visualize which parts of the image the model is focusing on to make its decision.\n",
    "\n",
    "**Harry**: Those all sound like great next steps! Let's implement fine-tuning real quick to see if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model):\n",
    "    \"\"\"Fine-tune the last few layers of the base model\"\"\"\n",
    "    # Get the base model (VGG16)\n",
    "    base_model = model.layers[0]\n",
    "    \n",
    "    # Unfreeze the last 4 convolutional layers\n",
    "    for layer in base_model.layers[-4:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Recompile with a very small learning rate\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),  # Even smaller learning rate\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Fine-tune the model\n",
    "model = fine_tune_model(model)\n",
    "\n",
    "# Continue training for a few more epochs\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=5,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot the fine-tuning history\n",
    "plot_training_history(fine_tune_history)\n",
    "\n",
    "# Test the fine-tuned model\n",
    "test_on_random_images(model, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: Did fine-tuning help?\n",
    "\n",
    "**Adil**: It often improves performance, especially on more nuanced aspects of our specific task. In our case, we already had high accuracy, so the improvement might be modest, but it could help with those challenging edge cases.\n",
    "\n",
    "**Harry**: So what else could we do with this model now that it's trained?\n",
    "\n",
    "**Adil**: Great question! Here are some fun applications:\n",
    "\n",
    "1. **Build a web app** that lets users upload their pet photos and get a classification\n",
    "2. **Extend it to more animals** by adding new categories like rabbits, hamsters, etc.\n",
    "3. **Deploy it on a mobile app** to classify animals in real-time from your phone camera\n",
    "4. **Use it for analyzing pet social media** to automatically tag photos\n",
    "\n",
    "Let's create a simple function that would let you classify any image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pet_image(model, image_path):\n",
    "    \"\"\"Classify a pet image as cat or dog\"\"\"\n",
    "    # Load and preprocess image\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "        img_array = np.array(img) / 255.0\n",
    "        \n",
    "        # Handle grayscale images by converting to RGB\n",
    "        if len(img_array.shape) == 2:\n",
    "            img_array = np.stack([img_array, img_array, img_array], axis=2)\n",
    "        elif img_array.shape[2] == 1:\n",
    "            img_array = np.concatenate([img_array, img_array, img_array], axis=2)\n",
    "        elif img_array.shape[2] == 4:  # Handle RGBA\n",
    "            img_array = img_array[:, :, :3]\n",
    "            \n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img_array)[0][0]\n",
    "        \n",
    "        # Determine class and confidence\n",
    "        pred_class = 'Dog' if prediction > 0.5 else 'Cat'\n",
    "        confidence = prediction if pred_class == 'Dog' else 1 - prediction\n",
    "        \n",
    "        # Display image with prediction\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img_array[0])\n",
    "        plt.title(f\"Prediction: {pred_class} with {confidence:.1%} confidence\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        return pred_class, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "# classify_pet_image(model, 'path/to/your/pet/image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harry**: This is so cool! I can now build my own pet classifier app!\n",
    "\n",
    "**Adil**: Exactly! And you could deploy this model using frameworks like Flask or FastAPI to create a web service, or TensorFlow Lite to run it on mobile devices.\n",
    "\n",
    "## ðŸŽ“ Conclusion\n",
    "\n",
    "**Adil**: So, what did you learn from this project, Harry?\n",
    "\n",
    "**Harry**: So much! I learned that:\n",
    "1. Transfer learning lets us leverage pre-trained models to solve new problems with less data and training time\n",
    "2. Data augmentation helps create a more robust model by artificially expanding our training data\n",
    "3. Convolutional Neural Networks are amazing at image recognition tasks\n",
    "4. The whole process is a lot more accessible than I thought - we built a pretty accurate model in just a few steps!\n",
    "\n",
    "**Adil**: Perfect summary! And remember, this is just the beginning. Deep learning is a vast field with endless possibilities. The concepts you learned today apply to many other problems beyond cats and dogs!\n",
    "\n",
    "**Harry**: Thanks for guiding me through this, Adil! Next time, let's try something even more ambitious - maybe building a system that can generate images of cats and dogs!\n",
    "\n",
    "**Adil**: That sounds like a great next project! We could explore Generative Adversarial Networks (GANs) or diffusion models. The AI adventure continues!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}